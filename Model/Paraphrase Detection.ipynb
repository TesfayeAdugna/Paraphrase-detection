{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Gizaw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#z import!\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Embedding, LSTM, Input, Lambda\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Training Data**\n",
    "\n",
    "The training data is loaded from the 'train.csv' file using Pandas. The dataset is then displayed to provide a quick overview of the initial rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(4076, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence1</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1726</td>\n",
       "      <td>Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1727</td>\n",
       "      <td>Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $2.5 billion.</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  \\\n",
       "0  1726   \n",
       "1  1727   \n",
       "\n",
       "                                                                                            Sentence1  \\\n",
       "0  Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.   \n",
       "1              Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $2.5 billion.   \n",
       "\n",
       "                                                                                                     Sentence2  \\\n",
       "0  Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.   \n",
       "1          Yucaipa bought Dominick's in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.   \n",
       "\n",
       "   Class  \n",
       "0      1  \n",
       "1      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('msr_paraphrase_train.csv')\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "print(f'shape{train_data.shape}')\n",
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Test Data**\n",
    "\n",
    "The test data is loaded from the 'test.csv' file using Pandas. The dataset is then displayed to offer an initial glimpse of the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(1725, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence1</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PCCW's chief operating officer, Mike Butcher, and Alex Arena, the chief financial officer, will report directly to Mr So.</td>\n",
       "      <td>Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The world's two largest automakers said their U.S. sales declined more than predicted last month as a late summer sales frenzy caused more of an industry backlash than expected.</td>\n",
       "      <td>Domestic sales at both GM and No. 2 Ford Motor Co. declined more than predicted as a late summer sales frenzy prompted a larger-than-expected industry backlash.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  \\\n",
       "0   1   \n",
       "1   2   \n",
       "\n",
       "                                                                                                                                                                           Sentence1  \\\n",
       "0                                                          PCCW's chief operating officer, Mike Butcher, and Alex Arena, the chief financial officer, will report directly to Mr So.   \n",
       "1  The world's two largest automakers said their U.S. sales declined more than predicted last month as a late summer sales frenzy caused more of an industry backlash than expected.   \n",
       "\n",
       "                                                                                                                                                          Sentence2  \\\n",
       "0                                                      Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So.   \n",
       "1  Domestic sales at both GM and No. 2 Ford Motor Co. declined more than predicted as a late summer sales frenzy prompted a larger-than-expected industry backlash.   \n",
       "\n",
       "   Class  \n",
       "0      1  \n",
       "1      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('./msr_paraphrase_test.csv')\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "print(f'shape{test_data.shape}')\n",
    "test_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting Training Sentences (Column 1)**\n",
    "\n",
    "The sentences from the first column of the training data are extracted and stored in the 'train_1' list. This list is then displayed using the print function, providing a sample of the sentences in the first column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train_data.iloc[:,1]\n",
    "train_1 = list(train_1)\n",
    "# print(train_1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting Training Sentences (Column 2)**\n",
    "\n",
    "Similarly, sentences from the second column of the training data are extracted and stored in the 'train_2' list. The content of this list is displayed using the print function, presenting a sample of sentences from the second column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2 = train_data.iloc[:,2]\n",
    "train_2 = list(train_2)\n",
    "# print(train_2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_train = train_1 + train_2\n",
    "# print(full_train[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Tokenization Setup**\n",
    "\n",
    "A Tokenizer is initialized with a vocabulary size of 5000 words. It is configured to filter out specific characters, convert text to lowercase, and split text based on predefined characters. This tokenizer will be used to convert textual data into numerical sequences for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 5000\n",
    "tokenizer = Tokenizer(num_words=num_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,split=' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization on Training Data**\n",
    "\n",
    "The Tokenizer is fitted on the entire training dataset (`full_train`), extracting unique tokens and building a vocabulary. The number of unique tokens found is printed along with the dictionary mapping words to their respective indices. This information is crucial for the subsequent conversion of text data into sequences of numerical values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13792 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(full_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "# print(word_index) # print the mapping between unique word and index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text to Sequence Conversion and Padding for Sentence1**\n",
    "\n",
    "The sentences from the 'Sentence1' column in the training data are converted into sequences of numerical values using the previously fitted tokenizer. The resulting sequences are then padded to a specified maximum length (`maxlen`) to ensure uniform dimensions. The printed output displays the original and padded sequences for the first sentence, providing insight into the preprocessing steps applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1558, 507, 28, 1693, 1397, 16, 221, 1, 946, 3, 4082, 28, 353]\n",
      "Padded Sequences: \n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0 1558  507   28 1693 1397   16  221    1  946\n",
      "    3 4082   28  353]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4076, 60)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1 = tokenizer.texts_to_sequences(train_data['Sentence1'].values)\n",
    "print(X_1[0])\n",
    "maxlen = 60\n",
    "X_1 = pad_sequences(X_1, maxlen=maxlen)\n",
    "print(\"Padded Sequences: \")\n",
    "# print(X_1)\n",
    "print(X_1[0])\n",
    "\n",
    "X_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text to Sequence Conversion and Padding for Sentence2**\n",
    "\n",
    "Similar to 'Sentence1', the sentences from the 'Sentence2' column in the data are converted into sequences of numerical values using the pre-fitted tokenizer. The resulting sequences are then padded to a specified maximum length (`maxlen`) to ensure uniform dimensions. The printed output displays the original and padded sequences for the first sentence, offering insight into the preprocessing steps applied to 'Sentence2'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2150, 2, 146, 20, 96, 1, 946, 1558, 507, 28, 1693, 3, 4082, 28, 353]\n",
      "Padded Sequences: \n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0 2150    2  146   20   96    1  946 1558  507   28 1693\n",
      "    3 4082   28  353]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4076, 60)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2 = tokenizer.texts_to_sequences(train_data['Sentence2'].values)\n",
    "print(X_2[0])\n",
    "maxlen = 60\n",
    "X_2 = pad_sequences(X_2, maxlen=maxlen)\n",
    "print(\"Padded Sequences: \")\n",
    "# print(X_2)\n",
    "print(X_2[0])\n",
    "\n",
    "X_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Data Splitting**\n",
    "\n",
    "The training data is split into training and validation sets using a specified portion (`training_portion`). The labels corresponding to the sentences are extracted from the fourth column of the dataset and stored in the variable 'y'. This step is crucial for training the model and evaluating its performance on unseen data during the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_portion = 0.8\n",
    "y = list(train_data.iloc[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Tokenization and Padding (Test Data - Sentence1)**\n",
    "\n",
    "For the test data, the sentences from 'Sentence1' are tokenized using the previously fitted tokenizer. The resulting sequences are then padded to ensure uniform length, with a maximum length specified by 'maxlen'. This processing is essential to prepare the test data for input into the trained model, maintaining consistency with the training data format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130, 496, 361, 1927, 6, 4477, 1, 130, 376, 361, 26, 162, 3987, 2, 60, 209]\n",
      "Padded Sequences: \n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0  130  496  361 1927    6 4477    1  130  376  361   26  162\n",
      " 3987    2   60  209]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1725, 60)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1 = tokenizer.texts_to_sequences(test_data['Sentence1'].values)\n",
    "print(X_test1[0])\n",
    "maxlen = 60\n",
    "X_test1 = pad_sequences(X_test1, maxlen=maxlen)\n",
    "print(\"Padded Sequences: \")\n",
    "# print(X_test1)\n",
    "print(X_test1[0])\n",
    "\n",
    "X_test1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Tokenization and Padding (Test Data - Sentence2)**\n",
    "\n",
    "Similarly, for the test data, the sentences from 'Sentence2' are tokenized using the previously fitted tokenizer. The resulting sequences are then padded to ensure uniform length, with a maximum length specified by 'maxlen'. This preprocessing step ensures that the test data is formatted appropriately for input into the trained model, maintaining consistency with the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[728, 130, 496, 361, 1927, 6, 157, 130, 376, 361, 4477, 26, 162, 2, 209]\n",
      "Padded Sequences: \n",
      "[[   0    0    0 ...  162    2  209]\n",
      " [   0    0    0 ...   55  126  464]\n",
      " [   0    0    0 ...  124    5  286]\n",
      " ...\n",
      " [   0    0    0 ...    1  282  406]\n",
      " [   0    0    0 ...   35 4964  614]\n",
      " [   0    0    0 ...    6 1350 1110]]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0  728  130  496  361 1927    6  157  130  376  361 4477\n",
      "   26  162    2  209]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1725, 60)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test2 = tokenizer.texts_to_sequences(test_data['Sentence2'].values)\n",
    "print(X_test2[0])\n",
    "maxlen = 60\n",
    "X_test2 = pad_sequences(X_test2, maxlen=maxlen)\n",
    "print(\"Padded Sequences: \")\n",
    "print(X_test2)\n",
    "print(X_test2[0])\n",
    "\n",
    "X_test2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train-Validation Data Splitting**\n",
    "\n",
    "The training dataset is split into training and validation sets to facilitate model training and evaluation. The split is performed based on the specified 'training_portion,' ensuring a portion of the data is reserved for validation. This division allows the model to learn from the training set and assess its performance on unseen data during validation, helping to prevent overfitting and ensure generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = int(len(X_1)*training_portion)\n",
    "\n",
    "X_train1 = X_1[:training_size,:]\n",
    "X_train2 = X_2[:training_size,:]\n",
    "y_train  = y[:training_size]\n",
    "X_val1   = X_1[training_size:,:]\n",
    "X_val2   = X_2[training_size:,:]\n",
    "y_val    = y[training_size:]\n",
    "y_test = test_data.iloc[:,3]\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3260, 60)\n",
      "(3260, 60)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3260"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train1.shape)\n",
    "print(X_train2.shape)\n",
    "len(y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Configuration Parameters**\n",
    "\n",
    "The following parameters are crucial for configuring the Siamese LSTM model:\n",
    "\n",
    "- `embedding_dim`: The dimensionality of the word embeddings. Adjusting this parameter can impact the model's ability to capture semantic relationships.\n",
    "\n",
    "- `lstm_out`: The number of LSTM units in the output layer. This parameter determines the complexity of the LSTM layer and influences the model's learning capacity.\n",
    "\n",
    "- `gradient_clipping_norm`: The normalization value for gradient clipping. This technique helps stabilize training by preventing exploding gradients.\n",
    "\n",
    "- `batch_size`: The number of samples used in each iteration during training. It affects the model's training speed and memory consumption.\n",
    "\n",
    "- `n_epoch`: The number of training epochs. An epoch represents one complete pass through the entire training dataset. Adjust this parameter based on training convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 40 #Change to observe effects\n",
    "lstm_out = 256\n",
    "gradient_clipping_norm = 2.50\n",
    "batch_size = 128\n",
    "n_epoch = 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Callback Configuration**\n",
    "\n",
    "The code sets up callbacks to monitor the model during training:\n",
    "\n",
    "- `ReduceLROnPlateau`: This callback dynamically adjusts the learning rate when a monitored metric plateaus. It helps improve convergence and training efficiency.\n",
    "\n",
    "- `EarlyStopping`: Monitors the validation loss and stops training when the loss stops decreasing, preventing overfitting.\n",
    "\n",
    "- `ModelCheckpoint`: Saves the model's weights during training based on the best validation loss. The saved model can be used for further analysis or deployment.\n",
    "\n",
    "These callbacks collectively enhance the training process, ensuring optimal model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.05,\n",
    "                              patience=5, min_lr=0.001)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
    "\n",
    "modelcheckpoint = ModelCheckpoint(\"weights.{epoch:02d}-{val_loss:.3f}.h5\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto',  save_freq='epoch')\n",
    "\n",
    "callbacks = [earlystop,modelcheckpoint,reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Siamese LSTM Model Overview**\n",
    "\n",
    "This code defines a Siamese LSTM model for paraphrase detection. It comprises:\n",
    "\n",
    "- **Inputs**: Two sequences processed by shared embedding and LSTM layers.\n",
    "\n",
    "- **Outputs**: Manhattan distance measures similarity between LSTM outputs.\n",
    "\n",
    "- **Compilation**: Adadelta optimizer, mean squared error loss, and accuracy metric.\n",
    "\n",
    "- **Summary**: Model architecture is summarized for quick reference.\n",
    "\n",
    "The Siamese LSTM detects paraphrases by learning sentence pair similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Gizaw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 60)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 60, 40)               200000    ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 256)                  304128    ['embedding[0][0]',           \n",
      "                                                                     'embedding[1][0]']           \n",
      "                                                                                                  \n",
      " lambda (Lambda)             (None, 1)                    0         ['lstm[0][0]',                \n",
      "                                                                     'lstm[1][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 504128 (1.92 MB)\n",
      "Trainable params: 304128 (1.16 MB)\n",
      "Non-trainable params: 200000 (781.25 KB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "\n",
    "left_input = Input(shape=(maxlen,), dtype='int32')\n",
    "right_input = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(num_words, embedding_dim, input_length=maxlen, trainable=False)\n",
    "\n",
    "# Embedded version of the inputs\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_lstm = LSTM(lstm_out)\n",
    "\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "\n",
    "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "\n",
    "malstm = Model([left_input, right_input], [malstm_distance])\n",
    "\n",
    "# Adadelta optimizer, with gradient clipping by norm\n",
    "# optimizer = Adadelta(clipnorm=gradient_clipping_norm,learning_rate=0.4,rho=0.95)\n",
    "optimizer = Adam(learning_rate=0.002)  # Lets use adam instaed!\n",
    "\n",
    "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Use tf.compat.v1.executing_eagerly_outside_functions instead of tf.executing_eagerly_outside_functions\n",
    "# tf.compat.v1.executing_eagerly_outside_functions\n",
    "\n",
    "print(malstm.summary())\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Siamese LSTM Model**\n",
    "\n",
    "The code trains the Siamese LSTM model using the fit() function. It takes training inputs (X_train1, X_train2), labels (y_train), and other parameters like batch size, epochs, and validation data.\n",
    "\n",
    "Callbacks, including early stopping, model checkpointing, and learning rate reduction, are employed during training.\n",
    "\n",
    "The training progress is stored in the malstm_trained variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "WARNING:tensorflow:From c:\\Users\\Gizaw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Gizaw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "26/26 [==============================] - 9s 263ms/step - loss: 0.2098 - accuracy: 0.6712 - val_loss: 0.1941 - val_accuracy: 0.7108 - lr: 0.0020\n",
      "Epoch 2/60\n",
      " 1/26 [>.............................] - ETA: 4s - loss: 0.1951 - accuracy: 0.6484"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gizaw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 4s 149ms/step - loss: 0.1925 - accuracy: 0.7147 - val_loss: 0.1891 - val_accuracy: 0.7279 - lr: 0.0020\n",
      "Epoch 3/60\n",
      "26/26 [==============================] - 4s 152ms/step - loss: 0.1865 - accuracy: 0.7233 - val_loss: 0.1898 - val_accuracy: 0.7304 - lr: 0.0020\n",
      "Epoch 4/60\n",
      "26/26 [==============================] - 5s 204ms/step - loss: 0.1844 - accuracy: 0.7252 - val_loss: 0.1889 - val_accuracy: 0.7353 - lr: 0.0020\n",
      "Epoch 5/60\n",
      "26/26 [==============================] - 5s 203ms/step - loss: 0.1840 - accuracy: 0.7362 - val_loss: 0.1889 - val_accuracy: 0.7255 - lr: 0.0020\n",
      "Epoch 6/60\n",
      "26/26 [==============================] - 5s 200ms/step - loss: 0.1794 - accuracy: 0.7411 - val_loss: 0.1907 - val_accuracy: 0.7218 - lr: 0.0020\n",
      "Epoch 7/60\n",
      "26/26 [==============================] - 5s 196ms/step - loss: 0.1806 - accuracy: 0.7466 - val_loss: 0.1926 - val_accuracy: 0.7316 - lr: 0.0020\n",
      "Epoch 8/60\n",
      "26/26 [==============================] - 5s 199ms/step - loss: 0.1776 - accuracy: 0.7549 - val_loss: 0.1920 - val_accuracy: 0.7243 - lr: 0.0020\n",
      "Epoch 9/60\n",
      "26/26 [==============================] - 5s 201ms/step - loss: 0.1780 - accuracy: 0.7497 - val_loss: 0.1889 - val_accuracy: 0.7230 - lr: 0.0020\n",
      "Epoch 10/60\n",
      "26/26 [==============================] - 5s 209ms/step - loss: 0.1744 - accuracy: 0.7583 - val_loss: 0.1899 - val_accuracy: 0.7206 - lr: 0.0010\n",
      "Epoch 11/60\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.1729 - accuracy: 0.7727 - val_loss: 0.1885 - val_accuracy: 0.7316 - lr: 0.0010\n",
      "Epoch 12/60\n",
      "26/26 [==============================] - 8s 322ms/step - loss: 0.1714 - accuracy: 0.7730 - val_loss: 0.1899 - val_accuracy: 0.7365 - lr: 0.0010\n",
      "Epoch 13/60\n",
      "26/26 [==============================] - 7s 269ms/step - loss: 0.1708 - accuracy: 0.7767 - val_loss: 0.1893 - val_accuracy: 0.7328 - lr: 0.0010\n",
      "Epoch 14/60\n",
      "26/26 [==============================] - 5s 211ms/step - loss: 0.1698 - accuracy: 0.7813 - val_loss: 0.1889 - val_accuracy: 0.7279 - lr: 0.0010\n",
      "Epoch 15/60\n",
      "26/26 [==============================] - 6s 219ms/step - loss: 0.1692 - accuracy: 0.7791 - val_loss: 0.1896 - val_accuracy: 0.7365 - lr: 0.0010\n",
      "Epoch 16/60\n",
      "26/26 [==============================] - 9s 335ms/step - loss: 0.1691 - accuracy: 0.7853 - val_loss: 0.1913 - val_accuracy: 0.7328 - lr: 0.0010\n",
      "Epoch 17/60\n",
      "26/26 [==============================] - 9s 333ms/step - loss: 0.1688 - accuracy: 0.7819 - val_loss: 0.1897 - val_accuracy: 0.7402 - lr: 0.0010\n",
      "Epoch 18/60\n",
      "26/26 [==============================] - 9s 338ms/step - loss: 0.1681 - accuracy: 0.7917 - val_loss: 0.1895 - val_accuracy: 0.7304 - lr: 0.0010\n",
      "Epoch 19/60\n",
      "26/26 [==============================] - 9s 348ms/step - loss: 0.1684 - accuracy: 0.7828 - val_loss: 0.1907 - val_accuracy: 0.7377 - lr: 0.0010\n",
      "Epoch 20/60\n",
      "26/26 [==============================] - 8s 324ms/step - loss: 0.1676 - accuracy: 0.7905 - val_loss: 0.1896 - val_accuracy: 0.7341 - lr: 0.0010\n",
      "Epoch 21/60\n",
      "26/26 [==============================] - 9s 341ms/step - loss: 0.1677 - accuracy: 0.7939 - val_loss: 0.1894 - val_accuracy: 0.7402 - lr: 0.0010\n",
      "Epoch 22/60\n",
      "26/26 [==============================] - 5s 177ms/step - loss: 0.1670 - accuracy: 0.7859 - val_loss: 0.1908 - val_accuracy: 0.7292 - lr: 0.0010\n",
      "Epoch 23/60\n",
      "26/26 [==============================] - 5s 200ms/step - loss: 0.1666 - accuracy: 0.7887 - val_loss: 0.1891 - val_accuracy: 0.7316 - lr: 0.0010\n",
      "Epoch 24/60\n",
      "26/26 [==============================] - 5s 205ms/step - loss: 0.1665 - accuracy: 0.7819 - val_loss: 0.1902 - val_accuracy: 0.7353 - lr: 0.0010\n",
      "Epoch 25/60\n",
      "26/26 [==============================] - 5s 199ms/step - loss: 0.1666 - accuracy: 0.7933 - val_loss: 0.1910 - val_accuracy: 0.7328 - lr: 0.0010\n",
      "Epoch 26/60\n",
      "26/26 [==============================] - 7s 260ms/step - loss: 0.1663 - accuracy: 0.7902 - val_loss: 0.1894 - val_accuracy: 0.7365 - lr: 0.0010\n",
      "Epoch 27/60\n",
      "26/26 [==============================] - 6s 229ms/step - loss: 0.1662 - accuracy: 0.7877 - val_loss: 0.1909 - val_accuracy: 0.7279 - lr: 0.0010\n",
      "Epoch 28/60\n",
      "26/26 [==============================] - 5s 213ms/step - loss: 0.1659 - accuracy: 0.7982 - val_loss: 0.1904 - val_accuracy: 0.7365 - lr: 0.0010\n",
      "Epoch 29/60\n",
      "26/26 [==============================] - 5s 206ms/step - loss: 0.1659 - accuracy: 0.7954 - val_loss: 0.1909 - val_accuracy: 0.7402 - lr: 0.0010\n",
      "Epoch 30/60\n",
      "26/26 [==============================] - 5s 203ms/step - loss: 0.1645 - accuracy: 0.8009 - val_loss: 0.1904 - val_accuracy: 0.7414 - lr: 0.0010\n",
      "Epoch 31/60\n",
      "26/26 [==============================] - 5s 197ms/step - loss: 0.1651 - accuracy: 0.7991 - val_loss: 0.1888 - val_accuracy: 0.7353 - lr: 0.0010\n",
      "Epoch 32/60\n",
      "26/26 [==============================] - 6s 220ms/step - loss: 0.1650 - accuracy: 0.7951 - val_loss: 0.1913 - val_accuracy: 0.7426 - lr: 0.0010\n",
      "Epoch 33/60\n",
      "26/26 [==============================] - 9s 337ms/step - loss: 0.1647 - accuracy: 0.8009 - val_loss: 0.1897 - val_accuracy: 0.7328 - lr: 0.0010\n",
      "Epoch 34/60\n",
      "26/26 [==============================] - 9s 338ms/step - loss: 0.1640 - accuracy: 0.8074 - val_loss: 0.1907 - val_accuracy: 0.7402 - lr: 0.0010\n",
      "Epoch 35/60\n",
      "26/26 [==============================] - 7s 261ms/step - loss: 0.1643 - accuracy: 0.7942 - val_loss: 0.1908 - val_accuracy: 0.7304 - lr: 0.0010\n",
      "Epoch 36/60\n",
      "26/26 [==============================] - 4s 138ms/step - loss: 0.1636 - accuracy: 0.8021 - val_loss: 0.1891 - val_accuracy: 0.7316 - lr: 0.0010\n",
      "Epoch 37/60\n",
      "26/26 [==============================] - 5s 200ms/step - loss: 0.1636 - accuracy: 0.7957 - val_loss: 0.1907 - val_accuracy: 0.7353 - lr: 0.0010\n",
      "Epoch 38/60\n",
      "26/26 [==============================] - 7s 273ms/step - loss: 0.1637 - accuracy: 0.8067 - val_loss: 0.1896 - val_accuracy: 0.7316 - lr: 0.0010\n",
      "Epoch 39/60\n",
      "26/26 [==============================] - 9s 338ms/step - loss: 0.1635 - accuracy: 0.8028 - val_loss: 0.1907 - val_accuracy: 0.7292 - lr: 0.0010\n",
      "Epoch 40/60\n",
      "26/26 [==============================] - 9s 342ms/step - loss: 0.1629 - accuracy: 0.8021 - val_loss: 0.1907 - val_accuracy: 0.7341 - lr: 0.0010\n",
      "Epoch 41/60\n",
      "26/26 [==============================] - 9s 330ms/step - loss: 0.1634 - accuracy: 0.8028 - val_loss: 0.1907 - val_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 41: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "malstm_trained = malstm.fit([X_train1, X_train2], np.array(y_train), batch_size=batch_size, epochs=n_epoch,\n",
    "                            validation_data=([X_val1, X_val2], np.array(y_val)), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Pre-trained Siamese LSTM Model Weights**\n",
    "\n",
    "The code loads pre-trained weights for a Siamese LSTM model from the file \"weights.01-0.26.h5\". After successful loading, it prints \"Loaded model from disk\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "malstm.load_weights(\"weights.11-0.189.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation on Validation Data**\n",
    "\n",
    "The code evaluates the Siamese LSTM model on the validation data ([X_val1, X_val2], np.array(y_val)) using the pre-defined loss function. The batch size for evaluation is set to 'batch_size'. The 'earlystop' callback is used during evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 73ms/step - loss: 0.1885 - accuracy: 0.7316\n",
      "[0.18854939937591553, 0.7316176295280457]\n"
     ]
    }
   ],
   "source": [
    "loss = malstm.evaluate([X_val1,X_val2], np.array(y_val), batch_size = batch_size, callbacks=[earlystop])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Model on Test Data**\n",
    "\n",
    "To assess the model's performance on the test set, we can use the `evaluate` function with the test data. The steps are as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 2s 78ms/step - loss: 0.1996 - accuracy: 0.6904\n",
      "Test Loss: [0.19960922002792358, 0.6904347538948059]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract ground truth labels from the test data\n",
    "y_test = test_data.iloc[:, 3]\n",
    "\n",
    "# Use the evaluate function with the test data\n",
    "test_loss = malstm.evaluate([X_test1, X_test2], np.array(y_test), batch_size=batch_size)\n",
    "\n",
    "# Print or use the test loss for further analysis\n",
    "print(\"Test Loss:\", test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Set Analysis**\n",
    "\n",
    "The model evaluation on the test set resulted in a loss of approximately 0.1996 and an accuracy of around 69.96%. The loss metric indicates the dissimilarity between the predicted and true labels, with lower values being desirable. The accuracy, representing the percentage of correctly classified instances, stands at 69.96%.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
